{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Sai Vishal/Downloads')\n",
    "f1=open(\"test_test.txt\",'r').readlines()\n",
    "f2=open(\"wiki_clean_duplicates.txt\",'w')\n",
    "a=set(f1)\n",
    "print(a)\n",
    "for w in a:\n",
    "    f2.write(w)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "os.chdir('C:/Users/Sai Vishal/Downloads')\n",
    "completed_lines_hash = set()\n",
    "f1=open(\"wiki_clean_file.txt\", \"r\")\n",
    "f2=open(\"remove_duplicates.txt\", \"w\")\n",
    "\n",
    "for line in f1:\n",
    "  \n",
    "  hashValue = hashlib.md5(line.rstrip().encode('utf-8')).hexdigest()\n",
    "  \n",
    "  if hashValue not in completed_lines_hash:\n",
    "    f2.write(line)\n",
    "    completed_lines_hash.add(hashValue)\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"E:/ADM/BERT_Team3\")\n",
    "#f1=open(\"final_output.txt\",w)\n",
    "\n",
    "file1 = set(line.strip() for line in open('single_words.txt', 'r', encoding=\"utf8\"))\n",
    "file2 = set(line.strip() for line in open('BERT_Vocabulary.txt', 'r', encoding=\"utf8\"))\n",
    "\n",
    "for line in file1 & file2:\n",
    "    if line:\n",
    "        print (line)\n",
    "\n",
    "#f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Sai Vishal/Downloads')\n",
    "f1=open(\"clean_puncs_2.txt\",'r')\n",
    "f2=open(\"file1.txt\",'w')\n",
    "for i in f1:\n",
    "    for j in i:\n",
    "        if j==\" \":\n",
    "            break\n",
    "    if j!=\" \":\n",
    "        f2.write(i)\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Sai Vishal/Downloads')\n",
    "f1=open(\"clean_puncs_2.txt\",'r')\n",
    "f2=open(\"file2.txt\",'w')\n",
    "for i in f1:\n",
    "    for j in i:\n",
    "        if j==\" \":\n",
    "            f2.write(i)\n",
    "            break\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Sai Vishal/Downloads')\n",
    "f1=open(\"file2.txt\",'r')\n",
    "f2=open(\"file3.txt\",'w')\n",
    "for i in f1:\n",
    "    for j in i.split():\n",
    "        f2.write(j)\n",
    "        f2.write('\\n')\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Sai Vishal/Downloads')\n",
    "f1=open('analysis_multiple_word.txt','r')\n",
    "f2=open('analysis_multiple_word_sort.txt','w')\n",
    "a=sorted(f1)\n",
    "s=\"\"\n",
    "for i in a:\n",
    "    s += i\n",
    "f2.write(s)\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from bert_serving.client import BertClient\n",
    "#bc = BertClient()\n",
    "os.chdir(r'E:/abc')\n",
    "f2=open('BERT_Vocabulary.txt', 'r', encoding=\"utf8\").read()\n",
    "all_stopwords = stopwords.words('english')\n",
    "from pathlib import Path\n",
    "for fileName in Path('.').glob('*.txt'):\n",
    "    a=str(fileName)\n",
    "    lines=\"\"\n",
    "    wikiword=\"\"\n",
    "    f1=open(\"out.txt\",\"a+\")\n",
    "    with open(str(fileName.absolute()),'r',encoding='utf-8') as one_text:\n",
    "        lines=one_text.read()\n",
    "        splitwords=lines.split(\" \")\n",
    "        for i in splitwords:\n",
    "            if \"wiki_\" in i.lower():\n",
    "                w=wikiword+i\n",
    "                x=w.rpartition(\"__\")[0].replace(\"__\", \".\").replace(\"Wiki.\",\"\").replace(\".\",\" \").replace(\"_\",\" \").lower() + '\\n'\n",
    "                for p in x:\n",
    "                    if p==\" \":\n",
    "                        break\n",
    "                if p!=\" \":\n",
    "                    if x not in all_stopwords:\n",
    "                         if x in f2:\n",
    "                            x=x.replace('\\n','')\n",
    "                            print(x)\n",
    "                            #vec=bc.encode([x])\n",
    "                            #c=str(vec.argmax())\n",
    "                            #f1.write(a+','x+'\\n'+c)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "'''import tensorflow\n",
    "import numpy as np\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()'''\n",
    "x=0\n",
    "os.chdir(r'E:/ADM/reddit')\n",
    "#f1=open('/home/cc/BERT_Vocabulary.txt', 'r', encoding=\"utf8\").read()\n",
    "for fileName in Path('.').glob('*.txt'):\n",
    "    f=open(str(fileName.absolute()),'r',encoding='utf-8').read().split(\" \")\n",
    "    a=str(fileName)\n",
    "    #f2=open(\"/home/cc/out.txt\",\"a+\")\n",
    "    for i in f:\n",
    "        if \"wiki_\" in i.lower():\n",
    "            x+=1\n",
    "            print(i,end=',')\n",
    "            print(x)\n",
    "            '''\n",
    "            x=i.rpartition(\"__\")[0].replace(\"Wiki__\",\"\").replace(\"_\",\" \").lower()\n",
    "            for p in x:\n",
    "                if p==\" \":\n",
    "                    break\n",
    "            if p!=\" \":\n",
    "                if x in f1:\n",
    "                    vec=bc.encode([x])\n",
    "                    c=str(vec.argmax())\n",
    "                    f2.write(a+','+x+','+c+'\\n') '''\n",
    "print(x)\n",
    "f2.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
